\documentclass[14pt]{extarticle}
\usepackage[left=3cm, right=3cm]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{nccmath}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[math]{cellspace}
    \cellspacetoplimit 4pt
    \cellspacebottomlimit 4pt
\usepackage{titletoc}
\usepackage{float}
\usepackage[ruled,longend]{algorithm2e}
\usepackage{imakeidx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue,
    linktoc=all
}

\linespread{1.2}
\title{{\Huge Market Basket Analysis}\\{\Large Master in Data Science}}
\bigskip
\author{\bigskip \\ \bigskip{\Large Alberto Bertoncini 983833}\\ \smallskip{\Large Massimo Cavagna 9838??}\\ \bigskip \href{https://github.com/Bertonc98/ProgettoAMD}{Github repository} }

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\section{Introduction}
The purpose of this paper is to present some of the techniques used in order to perform the so called "market/basket" analysis for a huge amount of data.
At first these techniques were exploited for the analysis of purchases in markets, trying to find some relationships among the goods bought by customers.
The idea behind these algorithms is to find associations between goods so that can be claimed that if a customer buy item A he is also likely to buy item B and vice versa.
This concept could be extended to association between sets of goods (not only single item pairs) and to generic items instead of just goods, so that,  in the end, the aim of the algorithms that will be presented is to find frequent sets of items in all the baskets available.
In particular, three main algorithms will be implemented:
\vspace{-0.2cm}\begin{itemize}[leftmargin=*]
\item[-] A-priori:
\vspace{-0.4cm}\begin{itemize}
\item[-] base
\vspace{-0.2cm}\item[-] PCY
\end{itemize}
\vspace{-0.4cm}\item[-] SON
\vspace{-0.4cm}\item[-] Toivonen
\end{itemize}
Once the frequents sets are found, it is also important to check if all the items in one of these sets are actually associated one another:
indeed, considering the environment these techniques come from, we could find that some goods, such as "milk" or "bread" are always frequent, but it cannot be claimed that there is a actual relationship with all the other items in the same frequent set, since it will be bought independently from the other.
\section{Dataset}
The dataset in analysis is called "\href{https://www.kaggle.com/ashirwadsangwan/imdb-dataset}{IMDb dataset}" (version 6) created by data collected from the homonymous site IMDb. This dataset contains information about movies, their ratings and workers that have taken part in them. The data is divided into several files to simplify the analysis over specific aspect.
\begin{itemize}[leftmargin=*]
\vspace{-0.4cm}\item[-]{\it title.akas.tsv} contains informations about the localized version of the movies.
\vspace{-0.4cm}\item[-]{\it title.basics.tsv} contains general information about the movies, not influenced by the localization.
\vspace{-0.4cm}\item[-]{\it title.principals.tsv} contains information about the cast and the crew for each movie.
\vspace{-0.4cm}\item[-]{\it title.ratings.tsv} contains the IMDb rating informations about the movies.
\vspace{-1.0cm}\item[-]{\it name.basics.tsv} contains informations about the cast and the crew.
\end{itemize}
The aim of the analysis presented is to find sets of actors that have frequently worked together so only a few of these datasets will be used: in particular {\it title.principal.tsv} from which is possible create baskets of actors for every movie and {\it name.basics.tsv} from which is possible retrieve the names of the actors from their IDs.
\section{Data structure}
{\it title.principal.tsv} is a tsv file with the subsequent structure:
\begin{itemize}[leftmargin=*]
\vspace{-0.4cm}\item[-]{\it tconst} (string) is an alphanumeric identifier of the movie.
\vspace{-0.4cm}\item[-]{\it ordering} (integer) is a number used to uniquely identify rows for a given movie.
\vspace{-0.4cm}\item[-]{\it nconst} (string) is an alphanumeric identifier of the cast/crew person.
\vspace{-0.4cm}\item[-]{\it category} (string) is the role of that person in the movie (a person can do different roles in the same movie).
\vspace{-0.4cm}\item[-]{\it job} (string) is a further specification of the role (can be empty, with the symbol "\textbackslash N").
\vspace{-0.4cm}\item[-]{\it characters} (string) is the name of the character played (can be empty, with the symbol"\textbackslash N").
\end{itemize}
There are 36.499.704 rows for 5.710.740 different movies.\\
Only rows with an actor are considered, so the analysis is done over 14.830.233 rows.\\
Movies without any registered actor need to be filtered, reducing the number of movies to 3.602.200.\\
In the end the number of different actors in this dataset is 1.868.056.\\
The size of {\it title.principal.tsv} is 1.6 GB.\\

\noindent
{\it name.basics.tsv} isa  tsv file with the subsequent structure:
\begin{itemize}[leftmargin=*]
\vspace{-0.4cm}\item[-]{\it nconst } (string) is an alphanumeric identifier of the cast/crew person.
\vspace{-1.1cm}\item[-]{\it primaryName } (string) the name of the person.
\vspace{-0.4cm}\item[-]{\it birthYear } (YYYY) year of birth.
\vspace{-0.4cm}\item[-]{\it deathYear  } (YYYY) year of death.
\vspace{-0.4cm}\item[-]{\it primaryProfession } (array of strings) the top 3 profession of the person.
\vspace{-1.0cm}\item[-]{\it knownForTitles  } (array of tconst) movies the person is known for (can be empty).
\end{itemize}
There are 9.711.022 rows in this file with only 3.625.895 people marked as actor/actress.
\section{Preprocessing}
These datasets need to be formatted into precise structure in order to apply the market/basket analysis algorithms. For each movie is created the basket (a list) containing all the IDs for the actors that have played a role in it. \\
The actors are identified, into the baskets, with an integer ID, obtained from their alphanumeric ID (nconsts).
\section{Algorithms applied}
The algorithms implemented aim to obtain the list of frequent itemsets (sets of actors) in our preprocessed baskets exploiting the property of monotonicity in different ways, from trivial form to more complex but more efficiently. Since these algorithm are created to work well over big amount of data is important underline the number of times an algorithm pass over the data, and the number of itemsets created to be marked frequent or not.\\
The most trivial approach is the A-priori algorithm, which scan multiple times the baskets in order to construct and filter itemsets of increasing size until there aren't more. In this way there are n pass over the data, which is a problem if the number of baskets grows. The number of candidates frequent itemsets is very high at every passage, because for every frequent itemset of size k need to create every possible combination of size k+1 from them.\\
An improvement of this algorithm is the PCY (Park, Chen, Yu) that exploit the free memory present in memory while counting the frequency of itemsets of size k, creating an hashmap in which counts the number of times an itemset of size k+1 is reached in baskets (is possible have collisions, but this approach aim to reduce the number of itemsets to be saved in memory and for which need to count the frequency, not to directly delete not frequent itemsets). The hashmap is compressed into a bitmap to save space and permitting to create only itemsets marked as possible frequent in this map. To reach frequent itemsets of size k need to pass k times over data, as the previous algorithm, but is possible reduce the candidates at every iteration.


\vspace{-0.2cm}\begin{itemize}[leftmargin=*]
\item[-] A-priori:
\vspace{-0.4cm}\begin{itemize}
\item[-] base
\vspace{-0.2cm}\item[-] PCY
\end{itemize}
\vspace{-0.4cm}\item[-] SON
\vspace{-0.4cm}\item[-] Toivonen
\end{itemize}
\section{Scaling of proposed solutions}
\section{Experiments}
\section{Results}
{\it I/We declare that this material, which I/We now submit for assessment, is entirely my/our own work and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of my/our work. I/We understand that plagiarism, collusion, and copying are grave and serious offences in the university and accept the penalties that would be imposed should I engage in plagiarism, collusion or copying. This assignment, or any part of it, has not been previously submitted by me/us or any other person for assessment on this or any other course of study.}
\begin{comment}
The report should contain the following information:

-the chosen dataset, and the parts of the latter which have been considered,
-how data have been organized,
-the applied pre-processing techniques,
-the considered algorithms and their implementations,
-how the proposed solution scales up with data size,
-a description of the experiments,
-comments and discussion on the experimental results.

\end{comment}
\end{document}

