{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xs1S3VcjVRk"
      },
      "source": [
        "import csv\n",
        "import json\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpEiOePhjVRo"
      },
      "source": [
        "def createBaskets(sourceFile, destFile):\n",
        "    tsv = csv.reader(open(sourceFile, encoding=\"utf-8\"), delimiter=\"\\t\")\n",
        "    print(sys.getsizeof(tsv))\n",
        "    c=0\n",
        "    keys = []\n",
        "    baskets = []\n",
        "    items = []\n",
        "    for row in tsv:\n",
        "        if row[3]==\"actor\" or row[3]==\"actress\":\n",
        "            \n",
        "            if row[2] not in items:\n",
        "                items.append(row[2])\n",
        "            pos = items.index(row[2])\n",
        "            \n",
        "            if row[0] in keys:\n",
        "                baskets[keys.index(row[0])] += [pos]\n",
        "            else:\n",
        "                keys += [row[0]]\n",
        "                baskets.append([pos])\n",
        "            if c==1000:\n",
        "                break;\n",
        "            c = c+1\n",
        "#     return keys, baskets\n",
        "    with open(destFile+\".csv\",\"w\") as f:\n",
        "        wr = csv.writer(f,delimiter=\";\")\n",
        "        for el in baskets:\n",
        "            wr.writerow(el)\n",
        "    with open(\"actorIdToInt.csv\",\"w\") as f:\n",
        "        wr = csv.writer(f,delimiter=\";\")\n",
        "        for el in items:\n",
        "            wr.writerow([el])\n",
        "    #json.dump(baskets, open(destFile+'.json', 'w'))\n",
        "    #json.dump(keys, open('actorIdToInt.json', 'w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lza-t6UhjVRr",
        "outputId": "dee7a29a-a486-417a-8c17-302da501a756"
      },
      "source": [
        "createBaskets(\"data.tsv\", \"preprocessed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqIy5YOPjVRt",
        "outputId": "cc720a9e-27a0-4f7a-ff45-1850bf934b8a"
      },
      "source": [
        "loaded = json.load(open(\"preprocessed.json\", encoding=\"utf-8\"))\n",
        "print(loaded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 1], [2, 3], [4], [5, 6, 7], [8], [9, 10], [11, 12], [13], [14, 15, 16, 17], [18], [19], [20], [21], [21], [21], [21], [22], [23, 21], [19], [23, 24, 21], [19], [19], [19], [19], [19], [19], [25, 26, 27], [21], [28, 29, 21, 30], [21], [31], [23], [21], [32, 33, 34, 35, 36], [37, 38], [39, 40], [41], [37], [39, 40], [21], [23, 21], [41], [39], [21], [42, 43, 44, 23, 45], [23, 21], [23, 21], [21], [21], [46, 47, 48, 49], [37, 50], [21], [21], [51], [52, 53, 54], [31], [31], [31], [55], [41], [56], [57], [58, 59, 60], [41], [61, 56], [62, 63], [51, 64], [37, 38], [65], [66, 67], [57], [57], [68, 69, 70, 71, 72, 73], [74], [74], [31], [75, 76], [77, 78, 79], [21], [75, 76], [80], [81], [82, 83, 84, 85, 86, 87, 88], [75, 76], [31], [89], [90], [91, 92], [38], [93, 94, 95], [21], [74], [21, 96, 43, 97], [62, 63, 98, 99], [79], [79], [21], [31], [31], [100], [79], [61], [101, 102, 103, 104, 105, 106, 107], [21], [108, 109], [79], [110, 111, 112, 91], [37], [102], [21], [19], [21, 113, 43, 114, 115], [116, 117], [118], [119, 120, 121], [21], [122, 123, 124, 41, 125], [126], [127, 128, 129, 130, 131, 132, 133, 134], [98, 62, 135, 63, 136, 137], [21, 138, 23, 139], [102, 123, 140], [141, 142], [114, 143, 144, 145], [114], [146, 147], [148, 149, 150, 151, 152, 153, 154, 155], [156, 157], [158], [159, 160, 161, 162, 163, 164, 165], [166, 167, 168, 169, 170, 171, 172, 173], [174, 175, 176, 177, 178, 179], [140, 125], [158], [180, 181], [182, 183], [180, 184, 185, 181], [186, 149, 154, 187], [188], [181, 180, 189], [185], [122, 190], [41], [191, 181, 189], [180, 181, 185], [192, 193, 194, 195, 196, 197, 198, 199], [200, 201], [202, 200, 201, 203], [21], [189, 185], [204, 205, 206, 207], [208, 21], [181, 189], [209], [102], [210], [211, 212, 213, 214], [185, 215, 216, 181, 189, 217, 218], [219], [220], [191, 181, 189, 221, 185, 222], [223, 224], [225, 226, 227, 228], [181, 189, 221, 229, 230], [181, 189, 221, 185, 231], [216, 181, 189, 185, 229, 232], [157], [21], [233, 181, 234], [235, 236, 182], [233, 181, 237, 238], [41], [185], [239, 223, 224], [240, 241], [200], [242, 243, 244, 245], [236, 182, 183], [246, 247, 248, 249, 250, 251, 252], [253, 254, 255, 256, 257, 258], [157], [259, 260, 261, 262, 263, 264], [185, 221, 181, 216], [265, 266, 267, 268], [181], [269, 270], [271, 272, 273, 240, 274], [275], [224], [276, 277, 224, 278], [279, 239, 280, 281, 282], [239, 281, 283, 278, 280], [245], [284, 158, 285, 286, 287, 288, 289], [122, 290, 283, 190, 291, 292, 293], [211], [294, 295, 296, 297], [298, 239, 299, 276, 277, 223, 281], [277, 239, 276, 223], [300, 301, 298, 278, 283, 280, 239, 302], [303, 239, 301, 277, 223, 279, 304, 278], [281, 239, 298, 278, 279, 283], [305, 306, 307], [304, 283, 280, 308, 301, 223, 281, 309], [304, 283, 278, 239, 308, 211, 301], [310, 311], [276, 301, 277, 224, 304], [276, 223], [312, 313], [223, 276], [224, 278, 239, 281, 301, 314, 223], [279, 281, 278, 283, 301, 314, 315], [316, 317, 318, 319, 320, 321, 289, 322], [277, 276], [298, 283, 323, 281, 314, 280, 301, 279], [21], [276, 239, 277], [304, 283, 239, 314, 301, 223, 279, 324], [325, 326, 327, 328], [304, 279, 283, 278, 239, 301, 223, 324], [247, 329], [330, 331, 332, 160, 333, 277], [158, 334], [182], [278, 335, 301, 279, 283, 304, 324], [158, 336, 240, 337, 289, 322], [338], [339, 158], [290, 312, 313, 340], [341, 342, 343, 344], [276, 277], [278, 277, 224], [324, 301, 278, 239, 299, 315, 298], [283, 304, 278, 301, 239, 345, 315, 281], [346, 278, 239, 279, 223, 301, 298], [275], [278, 301, 315, 281], [281, 278, 301, 239, 315, 279], [347], [348, 349, 350], [304, 281, 278, 211, 301, 279, 283, 351], [352, 351, 279, 298, 281, 239, 308], [149, 328, 353, 187], [304, 279, 354, 301, 281, 355, 356], [283, 301, 277, 309, 304, 278], [223, 357, 239, 358, 278, 279, 345, 301], [277, 223], [276], [276, 211, 277, 359, 224, 278], [351, 281, 278, 283, 301, 239, 279], [279, 283, 301, 356], [276, 277, 224, 304], [360, 361, 290, 190], [278, 276, 299, 277, 301], [278, 277, 239, 224], [277, 239, 304], [290, 275], [102], [276, 352, 224], [340, 190, 362, 122, 360, 363, 283], [301, 239, 281, 278], [276, 277, 301, 211, 224, 304], [167, 166], [364, 365, 366, 367, 368, 369], [370, 190, 340, 290, 122], [276, 223], [224, 314, 278, 276, 301, 223, 371], [281, 278, 301, 279, 283, 324, 304], [324, 314, 283, 304, 301, 281, 279], [372, 314, 283, 324, 357, 239, 224], [277], [276, 277, 304], [190, 370, 373, 122, 374, 291, 375, 376], [276], [377, 299, 276, 304, 224, 378, 379], [380], [309, 301, 239, 281, 279, 283], [279, 335, 278, 283, 301, 239, 281], [277, 276, 239], [223, 224, 277], [278, 283, 304, 276], [224, 283, 281, 239, 381, 301, 279], [281, 382, 239, 301, 278], [383, 384, 277, 385], [278, 190, 299, 290, 370, 386, 283, 122], [247, 248, 246, 249], [387, 283, 301, 280, 314, 359, 281, 279], [276, 211], [360, 122, 283, 374, 388, 362, 291, 389], [299, 276, 277, 224, 304, 278], [299, 190], [390, 391, 181, 189], [181, 391], [181, 392, 391, 393], [387, 314, 283, 239, 301, 223, 279, 309], [299, 223], [301, 283, 239, 223, 304, 278], [304, 277, 276, 239, 211, 301, 224], [275], [278, 283, 239, 280, 301, 314, 281], [391, 181, 189, 218, 229, 222, 232], [21], [283, 279, 239, 278], [301, 276, 299, 298, 278, 394], [298, 279, 283, 278, 301, 239, 223, 281], [224, 276, 211, 223, 304, 278], [177, 395], [396, 397, 398, 399], [312, 340], [283, 304, 279, 223, 345, 301, 359], [283, 281, 278, 239, 280]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "LYf8LEHnjVRt",
        "outputId": "370724b3-1f5c-4df3-f970-c78249d626ec"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz    \n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip3 install -q findspark\n",
        "!pip3 install -q pyspark\n",
        "!pip3 install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets files \"ashirwadsangwan/imdb-dataset\"\n",
        "!kaggle datasets download \"ashirwadsangwan/imdb-dataset\" -f \"title.principals.tsv.gz\"\n",
        "!unzip \"title.principals.tsv.gz.zip\"\n",
        "!rm \"title.principals.tsv.gz.zip\"\n",
        "!gunzip -k \"title.principals.tsv.gz\"\n",
        "!rm \"title.principals.tsv.gz\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5b65c5c1-b8dc-4b8b-9f91-bd8d1afadcb8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5b65c5c1-b8dc-4b8b-9f91-bd8d1afadcb8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "kaggle.json\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "name                      size  creationDate         \n",
            "-----------------------  -----  -------------------  \n",
            "title.principals.tsv.gz  290MB  2019-11-18 18:36:47  \n",
            "title.ratings.tsv.gz       5MB  2019-11-18 18:36:47  \n",
            "title.akas.tsv.gz        158MB  2019-11-18 18:36:47  \n",
            "name.basics.tsv.gz       180MB  2019-11-18 18:36:47  \n",
            "title.basics.tsv.gz      108MB  2019-11-18 18:36:47  \n",
            "Downloading title.principals.tsv.gz.zip to /content\n",
            " 96% 276M/287M [00:02<00:00, 128MB/s]\n",
            "100% 287M/287M [00:03<00:00, 98.7MB/s]\n",
            "Archive:  title.principals.tsv.gz.zip\n",
            "  inflating: title.principals.tsv.gz  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRV-pFU7nl8Q"
      },
      "source": [
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "home = os.environ[\"SPARK_HOME\"]\n",
        "os.environ[\"PYTHONPATH\"] = home+\"/python\"+home+\"/python/lib/py4j-0.10.9-src.zip:\"+os.environ[\"PYTHONPATH\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj2Stk8SjVRu"
      },
      "source": [
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImqZKjluxmRC"
      },
      "source": [
        "sourceFile = 'title.principals.tsv'\n",
        "\n",
        "data = spark.read.options(delimiter = \"\\t\", header=True).csv(sourceFile)\n",
        "\n",
        "rdd_result = data.rdd.map(lambda row: (row[\"nconst\"], 0) if row[\"category\"] == 'actress' or row[\"category\"] == 'actor' else (-1, 0)).reduceByKey(lambda a, b: 0)\n",
        "res = rdd_result.take(100)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNx2es-PyJjG"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(spark)\n",
        "df = sqlContext.createDataFrame(res, ['item', 'value']).select('item')\n",
        "df.coalesce(1).write.format('com.databricks.spark.csv').save(\"folder.csv\")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg9C8d_468yJ"
      },
      "source": [
        "!mv folder.csv/*.csv actorIdToInt.csv"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJU7dJnP1i-f"
      },
      "source": [
        "!rm -rf actorIdToInt.csv/"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikFnITsQxnJD"
      },
      "source": [
        "Create preprocessed file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUbdH6a5jVRv"
      },
      "source": [
        "sourceFile = 'title.principals.tsv'\n",
        "\n",
        "data = spark.read.options(delimiter = \"\\t\", header=True).csv(sourceFile)\n",
        "\n",
        "def filter_actors(row):\n",
        "    if row[\"category\"] == 'actress' or row[\"category\"] == 'actor':\n",
        "        return (row[\"tconst\"], [row[\"nconst\"]])\n",
        "\n",
        "rdd_result = data.rdd.map(lambda row: (row[\"tconst\"], [row[\"nconst\"]]) if row[\"category\"] == 'actress' or row[\"category\"] == 'actor' else (-1, [])).reduceByKey(lambda a, b: a + b)\n",
        "res = rdd_result.take(100)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep4hI4Htujj0"
      },
      "source": [
        "df = sqlContext.createDataFrame(res, ['movie_id', 'actors']).select('actors')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "qgsCfKKXvnhI",
        "outputId": "3750c36d-ff1c-45df-a237-c5c54e6ea70c"
      },
      "source": [
        "df.write.format('com.databricks.spark.csv').save(\"preprocessed.csv\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d4f77c08f9aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.databricks.spark.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preprocessed.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: CSV data source does not support array<string> data type."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fupxUeAikgyg",
        "outputId": "42572ebe-fa94-4fc2-e448-97b07dc9b0eb"
      },
      "source": [
        "def toCSVLine(data):\n",
        "  return ','.join(str(d[1]) for d in data)\n",
        "\n",
        "lines = labelsAndPredictions.map(toCSVLine)\n",
        "lines.saveAsTextFile('hdfs://my-node:9000/tmp/labels-and-predictions.csv')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-58b3e3ef5e04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 16) (ab499c6ebad8 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nTypeError: cannot unpack non-iterable NoneType object\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark-3.1.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nTypeError: cannot unpack non-iterable NoneType object\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    }
  ]
}